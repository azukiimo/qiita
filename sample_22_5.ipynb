{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19c2e8f",
   "metadata": {},
   "source": [
    "## ğŸ”°PyTorchã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºç¤ #22.5ã€MultiHeadAttentionã€‘\n",
    "\n",
    "* æ¬¡å›ï¼ˆæ³¨æ„è¡Œåˆ—ãƒ»attention weightsã®å¯è¦–åŒ–ï¼‰ã®æº–å‚™ç·¨\n",
    "* attention weightsã®æŠ½å‡ºæ–¹æ³•ã‚„è¨ˆç®—ã‚’è¦‹ã‚‹ã“ã¨ãŒä¸»ç›®çš„\n",
    "\n",
    "## MultiHeadAttention\n",
    "\n",
    "### è«–æ–‡ï¼ˆAttention Is All You Needï¼‰æµ\n",
    "è«–æ–‡ã§ã¯ã€MultiHeadAttentionã®å‡ºåŠ›ã‚’æ¬¡ã®æ‰‹é †ã§èª¬æ˜ã—ã¦ã„ã‚‹ã€‚\n",
    "1. åˆ†æ•£è¡¨ç¾è¡Œåˆ—$X$ã«$W^Q, W^K, W^V$è¡Œåˆ—ã‚’æ›ã‘ã¦ ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼è¡Œåˆ—$Q, K, V$ ã‚’è¨ˆç®—ã™ã‚‹ã€‚\n",
    "    * $Q = XW^Q$\n",
    "    * $K = XW^K$\n",
    "    * $V = XW^V$\n",
    "\n",
    "2. $Q, K, V$ã« $h$ï¼ˆãƒ˜ãƒƒãƒ‰ã®æ•°ï¼‰å€‹ã®è¡Œåˆ—$W_i^Q, W_i^K, W_i^V$ã‚’æ›ã‘ç®—ã—ã¦ã€$Q_i, K_i, V_i$ã‚’ç”¨æ„ã™ã‚‹ã€‚\n",
    "    * $Q_i = QW_i^Q$\n",
    "    * $K_i = KW_i^K$\n",
    "    * $V_i = VW_i^V$\n",
    "\n",
    "3. ãã‚Œãã‚Œã®$Q_i, K_i, V_i$ã§attentionã®è¨ˆç®—ã‚’è¡Œã†ã€‚\n",
    "    ```math\n",
    "    head_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_i K_i^T}{\\sqrt{d}}) V_i \n",
    "    ```\n",
    "\n",
    "4. ãƒ˜ãƒƒãƒ‰ã‚’é›†ã‚ã¦ã€$W^O$ã‚’æ›ã‘ç®—ã—ã€$X$ã¨åŒã˜å½¢çŠ¶ã§å‡ºåŠ›ã™ã‚‹ã€‚\n",
    "    ```math\n",
    "    \\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1,â€¦,head_h) W^O\n",
    "    ```\n",
    "\n",
    "\n",
    "Q, K, Vã«å„ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹è¡Œåˆ—ã‚’ç”¨ã„ã¦ Qi, Ki, Vi ã‚’ä½œã‚Šã€ãã‚Œãã‚Œã§æ³¨æ„æ©Ÿæ§‹ã‚’è¨ˆç®—ã—ã¦ã‹ã‚‰ concat ã™ã‚‹å½¢ã«ãªã£ã¦ã„ã‚‹ã€‚\n",
    "\n",
    "### PyTorchæµ\n",
    "\n",
    "PyTorch ã®å®Ÿè£…ã§ã¯ã€åŠ¹ç‡çš„ã«å®Ÿè£…ã—ã¦ã„ã‚‹ã€‚\n",
    "1. $W^Q, W^K, W^V$ ã‚’ç¸¦ã«çµåˆã—ãŸã‚ˆã†ãªè¡Œåˆ—$W$ã‚’åˆ†æ•£è¡¨ç¾è¡Œåˆ—$X$ã«æ›ã‘ã¦ $Q, K, V$ ã‚’ã¾ã¨ã‚ã¦è¨ˆç®—\n",
    "2. $Q, K, V$ã‚’ãƒ˜ãƒƒãƒ‰æ•°ã«ç­‰åˆ†å‰²ã—ã¦ $Q_i, K_i, V_i$ ã«å¯¾å¿œ\n",
    "3. åˆ†å‰²ã•ã‚ŒãŸå€¤ã§ãƒ˜ãƒƒãƒ‰ã”ã¨ã®æ³¨æ„æ©Ÿæ§‹ã‚’è¨ˆç®—ã— $Attention(Q_i, K_i, V_i)$\n",
    "4. å„ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’ concatã—ã¦ã€$Concat(head_1,...,head_h)W^O$ã‚’å‡ºåŠ›ã¨ã™ã‚‹\n",
    "\n",
    "### ç¢ºèªã—ãŸã„ã“ã¨\n",
    "* PyTorchã§attention weightsã‚’å–å¾—ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿã€1ã€‘\n",
    "* åˆ†å‰²ã•ã‚ŒãŸ$Q_1$ã€$K_1$ãªã©ã¯ã©ã‚Œï¼Ÿã€2ã€‘\n",
    "* ãƒ˜ãƒƒãƒ‰æ¯ã®æ³¨æ„è¡Œåˆ—ãƒ»attention weightsã¯ã©ã‚Œï¼Ÿã€3ã€‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d6685",
   "metadata": {},
   "source": [
    "### PyTorchã®MultiheadAttention\n",
    "\n",
    "* Documents: https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "\n",
    "* ã€1ã€‘mhaã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã€need_weights=Trueï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰ã€average_attn_weights=False ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ãƒ˜ãƒƒãƒ‰æ¯ã®é‡ã¿ãŒå–å¾—ã§ãã‚‹ã€‚\n",
    "* ã€3ã€‘mhaã®å‡ºåŠ›ã«ãƒ˜ãƒƒãƒ‰æ¯ã®é‡ã¿ãŒã‚ã‚‰ã‚ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2419a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(55) # torch.randnã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ãŸã„å ´åˆã¯å¤–ã™\n",
    "\n",
    "seq_len = 5\n",
    "embed_dim = 4\n",
    "num_heads = 2\n",
    "\n",
    "# ãƒ€ãƒŸãƒ¼å…¥åŠ› (batch, seq_len, embed_dim)\n",
    "X = torch.randn(1, seq_len, embed_dim)\n",
    "\n",
    "(1)\n",
    "mha = nn.MultiheadAttention(embed_dim, num_heads, bias=False, batch_first=True)\n",
    "mha.eval() # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«ã—ã¦dropoutãªã©ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æ’é™¤\n",
    "\n",
    "# (2)\n",
    "torch_output, torch_w = mha(query=X, key=X, value=X, average_attn_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e46fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_output:\n",
      "tensor([[[ 0.3529,  0.0220,  0.0969, -0.1303],\n",
      "         [ 0.4565,  0.1399,  0.0720,  0.1694],\n",
      "         [ 0.3354,  0.1571,  0.0336,  0.2145],\n",
      "         [ 0.3725,  0.0816,  0.1403, -0.0746],\n",
      "         [ 0.3248,  0.0932,  0.0436,  0.0879]]], grad_fn=<TransposeBackward0>)\n",
      "attention weights:\n",
      "tensor([[[[0.0424, 0.2048, 0.3446, 0.2414, 0.1667],\n",
      "          [0.1729, 0.1644, 0.2146, 0.2448, 0.2033],\n",
      "          [0.2667, 0.1591, 0.1675, 0.2068, 0.2000],\n",
      "          [0.0698, 0.2457, 0.3001, 0.2061, 0.1782],\n",
      "          [0.1792, 0.1710, 0.2114, 0.2354, 0.2030]],\n",
      "\n",
      "         [[0.1456, 0.1290, 0.2313, 0.2845, 0.2096],\n",
      "          [0.2896, 0.3155, 0.1334, 0.0994, 0.1622],\n",
      "          [0.2136, 0.3166, 0.1714, 0.1335, 0.1649],\n",
      "          [0.1254, 0.2581, 0.2383, 0.2125, 0.1655],\n",
      "          [0.1764, 0.2372, 0.2087, 0.1930, 0.1846]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch_output:\\n{torch_output}\")\n",
    "print(f\"attention weights:\\n{torch_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95238146",
   "metadata": {},
   "source": [
    "## MultiheadAttentionã®è¨ˆç®—é€”ä¸­ã‚’è¡¨ç¤ºã—ã¦ã¿ã‚‹\n",
    "mhaã§åˆ©ç”¨ã—ãŸå†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c396d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = mha.in_proj_weight      # (3*embed_dim, embed_dim) W_Q, W_K, W_Vã‚’åˆã‚ã›ãŸå¤§ããªè¡Œåˆ—\n",
    "\n",
    "E = mha.embed_dim           # mhaã§åˆ©ç”¨ã—ãŸåˆ†æ•£è¡¨ç¾ã®æ¬¡å…ƒ 4\n",
    "H = mha.num_heads           # mhaã§åˆ©ç”¨ã—ãŸãƒ˜ãƒƒãƒ‰æ•° 2\n",
    "D = E // H                  # ãƒ˜ãƒƒãƒ‰æ•°ã«åˆ†å‰²ã—ãŸæ™‚ã®åˆ†æ•£è¡¨ç¾ã®æ¬¡å…ƒ 2 \n",
    "\n",
    "# W = [W_Q, W_K, W_V]ã¨ã„ã†å¤§ããªè¡Œåˆ—ï¼ˆå®Ÿéš›ã¯ç¸¦ã ã‘ã©ï¼‰\n",
    "W_q = W[:E]\n",
    "W_k = W[E:2*E]\n",
    "W_v = W[2*E:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae14d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKVã‚’ä½œã‚‹Wã®å½¢çŠ¶ï¼štorch.Size([12, 4])\n",
      "W_qã®å½¢çŠ¶: torch.Size([4, 4])\n",
      "W_kã®å½¢çŠ¶: torch.Size([4, 4])\n",
      "W_vã®å½¢çŠ¶: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"QKVã‚’ä½œã‚‹Wã®å½¢çŠ¶ï¼š{W.shape}\")\n",
    "print(f\"W_qã®å½¢çŠ¶: {W_q.shape}\")\n",
    "print(f\"W_kã®å½¢çŠ¶: {W_k.shape}\")\n",
    "print(f\"W_vã®å½¢çŠ¶: {W_v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586ffb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV shape: torch.Size([1, 5, 12])\n",
      "QKV:\n",
      "tensor([[[-1.3839,  0.3560, -0.5477,  0.5145,  1.5560, -0.1749,  1.3026, -0.2896,  1.4396, -0.2397,  0.6415,  1.2935],\n",
      "         [-0.3053, -0.4555,  0.9167, -0.7092,  0.2180,  0.8775,  0.5869, -1.3853, -0.6356, -0.6922,  0.7399,  0.5402],\n",
      "         [ 0.1798, -0.4656,  0.2638, -0.6801, -0.4169,  0.4765,  0.0991, -0.2992, -0.8500, -0.1792, -0.0935, -0.1088],\n",
      "         [-0.8393,  0.6234, -0.7506, -0.4411, -0.1963, -0.0795,  0.0261,  0.1924, -0.3620,  1.1107, -0.1110,  0.4418],\n",
      "         [-0.2403, -0.3683, -0.1956, -0.2543,  0.2528,  0.1956,  0.6195, -0.0164, -0.0104, -0.3045, -0.0634,  0.2639]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(linewidth=200)\n",
    "\n",
    "QKV = F.linear(X, W)\n",
    "print(f\"QKV shape: {QKV.shape}\")\n",
    "print(f\"QKV:\\n{QKV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1eab83",
   "metadata": {},
   "source": [
    "### QKVè¡Œåˆ—ã®Q,K,Vã‚’ãƒ˜ãƒƒãƒ‰æ•°ã§åˆ†å‰²\n",
    "* ã€2ã€‘QKVã®è¡Œåˆ—ã‚’[Q1, Q2, K1, K2, V1, V2]ã¨åˆ†å‰²ã™ã‚Œã°è‰¯ã„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b1b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, Q2, K1, K2, V1, V2 = QKV.view(1,5,3*H,D).transpose(1,2).unbind(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342b9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1:\n",
      "tensor([[[-1.3839,  0.3560],\n",
      "         [-0.3053, -0.4555],\n",
      "         [ 0.1798, -0.4656],\n",
      "         [-0.8393,  0.6234],\n",
      "         [-0.2403, -0.3683]]], grad_fn=<UnbindBackward0>)\n",
      "Q2:\n",
      "tensor([[[-0.5477,  0.5145],\n",
      "         [ 0.9167, -0.7092],\n",
      "         [ 0.2638, -0.6801],\n",
      "         [-0.7506, -0.4411],\n",
      "         [-0.1956, -0.2543]]], grad_fn=<UnbindBackward0>)\n",
      "K1:\n",
      "tensor([[[ 1.5560, -0.1749],\n",
      "         [ 0.2180,  0.8775],\n",
      "         [-0.4169,  0.4765],\n",
      "         [-0.1963, -0.0795],\n",
      "         [ 0.2528,  0.1956]]], grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Qã®éƒ¨åˆ†ãŒ[Q1, Q2]ã¨ãªã£ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚æ®‹ã‚Šã‚‚åŒæ§˜ã§ã™ã€‚\n",
    "\n",
    "print(f\"Q1:\\n{Q1}\")\n",
    "print(f\"Q2:\\n{Q2}\")\n",
    "print(f\"K1:\\n{K1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526344e4",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒƒãƒ‰æ¯ã®attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f7e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax(Q1 K1 /âˆšd)\n",
    "head1_score = Q1@K1.transpose(-2,-1)/math.sqrt(D)\n",
    "head1_attn_weights = torch.softmax(head1_score, dim=-1)\n",
    "# softmax(Q1 K1 /âˆšd) V1\n",
    "head1 = head1_attn_weights@V1\n",
    "\n",
    "\n",
    "# softmax(Q2 K2 /âˆšd)\n",
    "head2_score = Q2@K2.transpose(-2,-1)/math.sqrt(D)\n",
    "head2_attn_weights = torch.softmax(head2_score, dim=-1)\n",
    "# softmax(Q2 K2 /âˆšd) V2\n",
    "head2 = head2_attn_weights@V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2754a",
   "metadata": {},
   "source": [
    "### mhaã§ã®é‡ã¿ã¨å€‹åˆ¥ã«è¨ˆç®—ã—ãŸé‡ã¿ã®å€¤ãŒç­‰ã—ã„ã“ã¨ã‚’ç›®è¦–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4506f6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0424, 0.2048, 0.3446, 0.2414, 0.1667],\n",
       "          [0.1729, 0.1644, 0.2146, 0.2448, 0.2033],\n",
       "          [0.2667, 0.1591, 0.1675, 0.2068, 0.2000],\n",
       "          [0.0698, 0.2457, 0.3001, 0.2061, 0.1782],\n",
       "          [0.1792, 0.1710, 0.2114, 0.2354, 0.2030]],\n",
       "\n",
       "         [[0.1456, 0.1290, 0.2313, 0.2845, 0.2096],\n",
       "          [0.2896, 0.3155, 0.1334, 0.0994, 0.1622],\n",
       "          [0.2136, 0.3166, 0.1714, 0.1335, 0.1649],\n",
       "          [0.1254, 0.2581, 0.2383, 0.2125, 0.1655],\n",
       "          [0.1764, 0.2372, 0.2087, 0.1930, 0.1846]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5993ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0424, 0.2048, 0.3446, 0.2414, 0.1667],\n",
       "          [0.1729, 0.1644, 0.2146, 0.2448, 0.2033],\n",
       "          [0.2667, 0.1591, 0.1675, 0.2068, 0.2000],\n",
       "          [0.0698, 0.2457, 0.3001, 0.2061, 0.1782],\n",
       "          [0.1792, 0.1710, 0.2114, 0.2354, 0.2030]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[0.1456, 0.1290, 0.2313, 0.2845, 0.2096],\n",
       "          [0.2896, 0.3155, 0.1334, 0.0994, 0.1622],\n",
       "          [0.2136, 0.3166, 0.1714, 0.1335, 0.1649],\n",
       "          [0.1254, 0.2581, 0.2383, 0.2125, 0.1655],\n",
       "          [0.1764, 0.2372, 0.2087, 0.1930, 0.1846]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head1_attn_weights, head2_attn_weights, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6dbadc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.stack([head1_attn_weights, head2_attn_weights], dim=1)\n",
    "print(torch.allclose(attn_weights, torch_w, atol=1e-6))  # True ã«ãªã‚‹ã¯ãš"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8c38c",
   "metadata": {},
   "source": [
    "### mhaã®å‡ºåŠ›å€¤ã¨å€‹åˆ¥è¨ˆç®—ã—ãŸå€¤ãŒç­‰ã—ã„ã“ã¨ã®ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7faa3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mhaã®å‡ºåŠ›ï¼š\n",
      "tensor([[[ 0.3529,  0.0220,  0.0969, -0.1303],\n",
      "         [ 0.4565,  0.1399,  0.0720,  0.1694],\n",
      "         [ 0.3354,  0.1571,  0.0336,  0.2145],\n",
      "         [ 0.3725,  0.0816,  0.1403, -0.0746],\n",
      "         [ 0.3248,  0.0932,  0.0436,  0.0879]]], grad_fn=<TransposeBackward0>)\n",
      "å€‹åˆ¥è¨ˆç®—ã®å‡ºåŠ›ï¼š\n",
      "tensor([[[ 0.3529,  0.0220,  0.0969, -0.1303],\n",
      "         [ 0.4565,  0.1399,  0.0720,  0.1694],\n",
      "         [ 0.3354,  0.1571,  0.0336,  0.2145],\n",
      "         [ 0.3725,  0.0816,  0.1403, -0.0746],\n",
      "         [ 0.3248,  0.0932,  0.0436,  0.0879]]], grad_fn=<UnsafeViewBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "head1 = head1_attn_weights@V1 # Attention(Q1,K1,V1)\n",
    "head2 = head2_attn_weights@V2 # Attention(Q2,K2,V2)\n",
    "\n",
    "concat_head = torch.concat([head1, head2], axis=2)\n",
    "output = F.linear(concat_head, mha.out_proj.weight)\n",
    "\n",
    "print(f\"mhaã®å‡ºåŠ›ï¼š\\n{torch_output}\")\n",
    "print(f\"å€‹åˆ¥è¨ˆç®—ã®å‡ºåŠ›ï¼š\\n{output}\")\n",
    "\n",
    "print(torch.allclose(output, torch_output , atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9ed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1ab572",
   "metadata": {},
   "source": [
    "## è«–æ–‡é¢¨ã®å®Ÿè£…\n",
    "ãƒ™ãƒƒãƒ‰æ¯ã«è¡Œåˆ—ã‚’ä½œæˆ\n",
    "* $Q_i = QW_i^Q$\n",
    "* $K_i = KW_i^K$\n",
    "* $V_i = VW_i^V$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{softmax}(\\frac{Q_i^Q K_i^T}{\\sqrt{d}}) V_i \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "MultiHead(Q,K,V) = Concat(head_1,â€¦,head_h) W^O\n",
    "$$\n",
    "\n",
    "**ã‚³ãƒ¼ãƒ‰ã®æµã‚Œ**\n",
    "* ãƒ˜ãƒƒãƒ‰ãŒ2ã¤ãªã®ã§Q,K,Vç”¨ã®é‡ã¿ã‚’(W_q1, W_k1, W_v1)ã¨(W_q2, , W_k2,  W_v2)ã¨2çµ„æº–å‚™ã™ã‚‹ã€‚\n",
    "* é‡ã¿è¡Œåˆ—ã‚’åˆ©ç”¨ã—ã¦ãƒ˜ãƒƒãƒ‰æ¯ã®QKVã‚’(Q1, K1, V1)ã¨(Q2, K2, V2)ã¨2çµ„è¨ˆç®—ã™ã‚‹ã€‚\n",
    "* Scaled Dot Product Attentionã‚’è¨ˆç®—ã€‚head1ã¨head2ãŒã§ãã‚‹\n",
    "* [head1, head2]ã‚’ä¸¦ã¹ã¦ã€å‡ºåŠ›ç”¨ã®é‡ã¿$W^O$ (mha.out_proj.weight) ã‚’æ›ã‘ç®—ã—ã¦æœ€çµ‚å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f45481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([1, 5, 4])\n",
      "W_q1 shape: torch.Size([2, 4])\n",
      "ãƒ˜ãƒƒãƒ‰ï¼‘ç”¨ã®ã‚¯ã‚¨ãƒªè¡Œåˆ—\n",
      "Q1 shape: torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, Vã®ä½œæˆ\n",
    "Q = X\n",
    "K = X\n",
    "V = X\n",
    "\n",
    "# å°ã•ãªè¡Œåˆ—ã‚’å…ˆã«ä½œæˆ\n",
    "# mhaã®Wã‚’åˆ©ç”¨ã—ã¦Q1ãªã©ã‚’ä½œæˆã™ã‚‹è¡Œåˆ—ã‚’ä½œã‚Šã¾ã™\n",
    "W_q1, W_q2 = W_q[:2,:], W_q[2:,:]\n",
    "W_k1, W_k2 = W_k[:2,:], W_k[2:,:]\n",
    "W_v1, W_v2 = W_v[:2,:], W_v[2:,:]\n",
    "\n",
    "# å€‹åˆ¥ã«Qiã‚’ä½œæˆ\n",
    "# ãƒ˜ãƒƒãƒ‰æ¯ã®QKV\n",
    "Q1 = F.linear(Q, W_q1)    # Q1 = Q W_q1^T\n",
    "Q2 = F.linear(Q, W_q2)\n",
    "K1 = F.linear(K, W_k1)\n",
    "K2 = F.linear(K, W_k2)\n",
    "V1 = F.linear(V, W_v1)\n",
    "V2 = F.linear(V, W_v2)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"W_q1 shape: {W_q1.shape}\")\n",
    "print(\"ãƒ˜ãƒƒãƒ‰ï¼‘ç”¨ã®ã‚¯ã‚¨ãƒªè¡Œåˆ—\")\n",
    "print(f\"Q1 shape: {Q1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7764db",
   "metadata": {},
   "source": [
    "* å„ãƒ˜ãƒƒãƒ‰ã«ã¤ã„ã¦Scaled Dot-Product Attention ã‚’è¨ˆç®—ã™ã‚‹ã€‚ã“ã‚ŒãŒ head_1, head_2\n",
    "* concat(head_1, head_2)@W^Oã«ã‚ˆã£ã¦æœ€çµ‚å‡ºåŠ›ã¨ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2002fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax(Q1 K1 /âˆšd) V1ã®éƒ¨åˆ†\n",
    "head1_score = Q1@K1.transpose(-2,-1)/math.sqrt(D)\n",
    "head1_attn_weights = torch.softmax(head1_score, dim=-1)\n",
    "head1 = head1_attn_weights@V1\n",
    "\n",
    "head2_score = Q2@K2.transpose(-2,-1)/math.sqrt(D)\n",
    "head2_attn_weights = torch.softmax(head2_score, dim=-1)\n",
    "head2 = head2_attn_weights@V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b888893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "concat_head = torch.concat([head1, head2], axis=2)\n",
    "output = F.linear(concat_head, mha.out_proj.weight)\n",
    "print(torch.allclose(output, torch_output , atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66a6b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3529,  0.0220,  0.0969, -0.1303],\n",
       "         [ 0.4565,  0.1399,  0.0720,  0.1694],\n",
       "         [ 0.3354,  0.1571,  0.0336,  0.2145],\n",
       "         [ 0.3725,  0.0816,  0.1403, -0.0746],\n",
       "         [ 0.3248,  0.0932,  0.0436,  0.0879]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mhaã®å‡ºåŠ›\n",
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a02d3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3529,  0.0220,  0.0969, -0.1303],\n",
       "         [ 0.4565,  0.1399,  0.0720,  0.1694],\n",
       "         [ 0.3354,  0.1571,  0.0336,  0.2145],\n",
       "         [ 0.3725,  0.0816,  0.1403, -0.0746],\n",
       "         [ 0.3248,  0.0932,  0.0436,  0.0879]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# é †ç•ªã«è¨ˆç®—ã—ãŸå‡ºåŠ›\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df44e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c27c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

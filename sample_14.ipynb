{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”°PyTorchã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºç¤ #14 ã€éŸ³æ¥½åˆ†é¡02ãƒ»conv1dã€‘\n",
    "\n",
    "## å†…å®¹\n",
    "* Qiitaã®è¨˜äº‹ã¨é€£å‹•ã—ã¦ã„ã¾ã™\n",
    "* 1æ¬¡å…ƒç•³ã¿è¾¼ã¿ã‚’åˆ©ç”¨ã—ãŸéŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ«åˆ†é¡\n",
    "\n",
    "## æ³¨æ„\n",
    "* [Kaggle: GTZAN Dataset - Music Genre Classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)ã‹ã‚‰GTZANãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "* ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆtrain_data_10.npzã‚’ä½œæˆã—ãªã„ã¨å‹•ä½œã—ãªã„ã®ã§æ³¨æ„ã—ã¦ãã ã•ã„\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦\n",
    "* train_data_10.npz : å„90ãƒ•ã‚¡ã‚¤ãƒ«ã€10ç§’ã€10åˆ†é¡\n",
    "* sampling rate 22050\n",
    "* ç³»åˆ—é•·ã®æ™‚é–“ï¼š10ç§’ (22050*10)\n",
    "\n",
    "### ãƒ†ã‚¹ãƒˆç²¾åº¦\n",
    "* è¨“ç·´ç²¾åº¦ï¼š99%\n",
    "* ãƒ†ã‚¹ãƒˆç²¾åº¦ï¼š70ã€œ75%ã€€ï¼ˆçµæ§‹å¤§ããå¤‰ã‚ã‚Šã¾ã™ã€‚è‰²ã€…å·¥å¤«ã™ã‚‹ä½™åœ°ã‚ã‚Šï¼‰\n",
    "* å­¦ç¿’ãƒ«ãƒ¼ãƒ—100å›ã§ã™ãŒã€ã‚‚ã†å°‘ã—ã‚ã£ãŸã»ã†ãŒã„ã„ã‹ã‚‚ğŸ˜“\n",
    "\n",
    "### ãã®ä»–\n",
    "* GPUåˆ©ç”¨æ™‚ã§ç´„20åˆ†\n",
    "* Google Colabã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚\n",
    "* ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã‚’è¡¨ç¤ºã•ã›ã‚‹torchinfoãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†å ´åˆã¯äº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚\n",
    "> pip install torchinfo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset  # ãƒŸãƒ‹ãƒãƒƒãƒã®åˆ©ç”¨\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2691, 220500), (2691,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"./data/train_data_10.npz\")\n",
    "x = data[\"x\"]\n",
    "t = data[\"t\"]\n",
    "x.shape, t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "* ãƒ‡ãƒ¼ã‚¿ã‚’torch.Tensorã«å¤‰æ›´\n",
    "* train_test_splitã§å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²\n",
    "* DataLoaderã§ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "X = torch.FloatTensor(x).view(x.shape[0], 1, x.shape[1])\n",
    "T = torch.LongTensor(t)\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2, stratify=t, random_state=55)\n",
    "\n",
    "# ãƒŸãƒ‹ãƒãƒƒãƒã«åŒºåˆ†ã‘ã™ã‚‹\n",
    "mini_batch_size = 100\n",
    "train_data = TensorDataset(x_train, t_train)\n",
    "train_loader = DataLoader(train_data, batch_size=mini_batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ \n",
    "* nn.Sequentialã‚’åˆ©ç”¨ã—ã¦ç‰¹å¾´é‡æŠ½å‡ºãƒ–ãƒ­ãƒƒã‚¯ã¨åˆ†é¡ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã¦è¨˜è¿°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.features = nn.Sequential(\n",
    "            # ç¬¬1ãƒ–ãƒ­ãƒƒã‚¯\n",
    "            nn.Conv1d(1, 64, kernel_size=1000, stride=16),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
    "            \n",
    "            # ç¬¬2ãƒ–ãƒ­ãƒƒã‚¯\n",
    "            nn.Conv1d(64, 128, kernel_size=500, stride=8),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=2),\n",
    "            \n",
    "            # ç¬¬3ãƒ–ãƒ­ãƒƒã‚¯\n",
    "            nn.Conv1d(128, 256, kernel_size=16, stride=4),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(8)  # å›ºå®šã‚µã‚¤ã‚ºã®å‡ºåŠ›\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(1000,), stride=(16,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(64, 128, kernel_size=(500,), stride=(8,))\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv1d(128, 256, kernel_size=(16,), stride=(4,))\n",
       "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): AdaptiveAvgPool1d(output_size=8)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.3, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®è¡¨ç¤º\n",
    "* torchinfoãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åˆ©ç”¨ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DNN                                      [1, 10]                   --\n",
       "â”œâ”€Sequential: 1-1                        [1, 256, 8]               --\n",
       "â”‚    â””â”€Conv1d: 2-1                       [1, 64, 13719]            64,064\n",
       "â”‚    â””â”€BatchNorm1d: 2-2                  [1, 64, 13719]            128\n",
       "â”‚    â””â”€ReLU: 2-3                         [1, 64, 13719]            --\n",
       "â”‚    â””â”€MaxPool1d: 2-4                    [1, 64, 6858]             --\n",
       "â”‚    â””â”€Conv1d: 2-5                       [1, 128, 795]             4,096,128\n",
       "â”‚    â””â”€BatchNorm1d: 2-6                  [1, 128, 795]             256\n",
       "â”‚    â””â”€ReLU: 2-7                         [1, 128, 795]             --\n",
       "â”‚    â””â”€MaxPool1d: 2-8                    [1, 128, 396]             --\n",
       "â”‚    â””â”€Conv1d: 2-9                       [1, 256, 96]              524,544\n",
       "â”‚    â””â”€BatchNorm1d: 2-10                 [1, 256, 96]              512\n",
       "â”‚    â””â”€ReLU: 2-11                        [1, 256, 96]              --\n",
       "â”‚    â””â”€AdaptiveAvgPool1d: 2-12           [1, 256, 8]               --\n",
       "â”œâ”€Sequential: 1-2                        [1, 10]                   --\n",
       "â”‚    â””â”€Flatten: 2-13                     [1, 2048]                 --\n",
       "â”‚    â””â”€Linear: 2-14                      [1, 512]                  1,049,088\n",
       "â”‚    â””â”€BatchNorm1d: 2-15                 [1, 512]                  1,024\n",
       "â”‚    â””â”€ReLU: 2-16                        [1, 512]                  --\n",
       "â”‚    â””â”€Dropout: 2-17                     [1, 512]                  --\n",
       "â”‚    â””â”€Linear: 2-18                      [1, 128]                  65,664\n",
       "â”‚    â””â”€BatchNorm1d: 2-19                 [1, 128]                  256\n",
       "â”‚    â””â”€ReLU: 2-20                        [1, 128]                  --\n",
       "â”‚    â””â”€Dropout: 2-21                     [1, 128]                  --\n",
       "â”‚    â””â”€Linear: 2-22                      [1, 10]                   1,290\n",
       "==========================================================================================\n",
       "Total params: 5,802,954\n",
       "Trainable params: 5,802,954\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.88\n",
       "Forward/backward pass size (MB): 16.08\n",
       "Params size (MB): 23.21\n",
       "Estimated Total Size (MB): 40.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, (1,1,220500))  # (ãƒãƒƒãƒã‚µã‚¤ã‚º, ãƒãƒ£ãƒ³ãƒãƒ«æ•°, ç³»åˆ—é•·)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå¤±é–¢æ•°ã¨æœ€é©åŒ–é–¢æ•°ã®å®šç¾©\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    _,argmax_list = torch.max(y, dim=1)\n",
    "    accuracy = sum(argmax_list == t).item()/len(t)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19: loss: 0.4493510623772939,\tacc:0.927142857142857\n",
      "39: loss: 0.1916269390355973,\tacc:0.9638095238095238\n",
      "59: loss: 0.08933362843734878,\tacc:0.9880952380952381\n",
      "79: loss: 0.06818140094124135,\tacc:0.9904761904761905\n",
      "99: loss: 0.032543914614333994,\tacc:0.9952380952380951\n"
     ]
    }
   ],
   "source": [
    "LOOP = 100\n",
    "model.train()\n",
    "for epoch in range(LOOP):\n",
    "    # ãƒŸãƒ‹ãƒãƒƒãƒã®å‡¦ç†\n",
    "    total_loss = 0  # æå¤±ã®ç´¯è¨ˆã‚’è¨ˆç®—\n",
    "    total_acc = 0   # ç²¾åº¦ã®ç´¯è¨ˆã‚’è¨ˆç®—\n",
    "    cnt = 0   # ãƒŸãƒ‹ãƒãƒƒãƒã§ã®ç¹°ã‚Šè¿”ã—å›æ•° cntã§å‰²ã‚Œã°å¹³å‡ã«ãªã‚‹\n",
    "    \n",
    "    for x, t in train_loader:\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        y = model(x)\n",
    "        loss = criterion(y, t)\n",
    "        acc  = accuracy(y, t)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        cnt += 1\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%20 == 0:\n",
    "        print(f\"{epoch}: loss: {total_loss/cnt},\\tacc:{total_acc/cnt}\")   # æå¤±ã¨ç²¾åº¦ã®è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7495361781076066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    x_test = x_test.to(device)\n",
    "    t_test = t_test.to(device)\n",
    "    y_test = model(x_test)\n",
    "test_acc = accuracy(y_test, t_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b658f575",
   "metadata": {},
   "source": [
    "## ğŸ”°PyTorchã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºç¤ #22ã€æ–‡ç« åˆ†é¡ãƒ»Transformerã€‘\n",
    "\n",
    "### å†…å®¹\n",
    "* Qiitaã®è¨˜äº‹ã¨é€£å‹•ã—ã¦ã„ã¾ã™\n",
    "* Transformerã‚’åˆ©ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡\n",
    "* åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼šdata/data_90.zip\n",
    "* data_90.zipã¯è§£å‡ã—ã¦åˆ©ç”¨\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦\n",
    "* webã‚µã‚¤ãƒˆã¸ã®ã€Œãƒ­ã‚°ã‚¤ãƒ³ã€ç™»éŒ²ã€è§£ç´„ã€ã®ï¼“ç¨®é¡ã®å•ã„åˆã‚ã›ã‚’åˆ†é¡ã™ã‚‹60å€‹ãƒ‡ãƒ¼ã‚¿\n",
    "* äº‹å‰ã«mecabãªã©ã§å½¢æ…‹ç´ è§£æã‚’ã—ã¦å˜èªã«IDã‚’å‰²ã‚ŠæŒ¯ã‚Šã€ç­‰é•·ã®é•·ã•ã«å¤‰æ›ã—ã¦ã‚ã‚‹\n",
    "* <bos>æ–‡ç« <eos><pad>ã®å½¢ã§é•·ã•ãŒ11ã«ãªã‚‹ã¾ã§<pad>ã‚’æŒ¿å…¥\n",
    "* ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°ï¼šï¼“\n",
    "* å˜èªæ•°ï¼š146\n",
    "* ç³»åˆ—é•·ï¼ˆæ–‡ç« ã®é•·ã•ï¼‰ï¼š11\n",
    "\n",
    "**Transformer Encoderã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®åŸºæœ¬æ§‹é€ **\n",
    "* åŸ‹ã‚è¾¼ã¿å±¤â†’TransformerEncoderå±¤â†’<BOS>ã¸é›†ç´„â†’FC\n",
    "\n",
    "**å‚è€ƒ**\n",
    "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã¯BERTé¢¨ã«ã—ã¦ã¿ã¾ã—ãŸã€‚\n",
    "* Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2018) \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
    "\n",
    "**ã€æ¤œè¨¼ç²¾åº¦ã€‘**\n",
    "* æ¤œè¨¼ç²¾åº¦: 0.9\n",
    "* D_MODEL = 16\n",
    "* head = 4\n",
    "* layer=6\n",
    "* dim_feed=32\n",
    "\n",
    "\n",
    "### æ¤œè¨¼ç²¾åº¦ ãŸã¾ãŸã¾ã»ã¨ã‚“ã©ãŒ70ã€œ80%ã€€æ‚ªã„ã¨60%ã«ãªã‚‹ã\n",
    "* 10å›ãã‚‰ã„è©¦ã—ãŸã‘ã©ã€90%ã¯1å›ã ã‘ã€ã»ã¨ã‚“ã©70ã€œ80%ã®ç¯„å›²ã€‚\n",
    "* æ•°å€¤ã‚’å¤§ããã™ã‚‹ã¨ã»ã‚“ã®ã™ã“ã—ã ã‘ç²¾åº¦ä¸Šæ˜‡ã£ã½ã„ãŒã€1ã€œ2å€‹å¤šãå½“ãŸã‚‹ã ã‘ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿23å€‹ã ã—ã­ã€‚\n",
    "\n",
    "### å‚¾å‘\n",
    "* layeræ•°ã‚’å¤§ããã™ã‚‹ã¨ç²¾åº¦ãŒä¸Šæ˜‡ã—ã‚„ã™ã„ã‘ã©ã€å¤§ãã™ãã¦ã‚‚NGã£ã½ã„\n",
    "* åŸºæœ¬çš„ã«æ•°å€¤ãŒå¤§ãã„ã»ã©ç²¾åº¦ã‚‚ç·©ã‚„ã‹ãªä¸Šæ˜‡å‚¾å‘ã«ã‚ã‚‹ã‘ã©ã€è¨ˆç®—é€Ÿåº¦ã‚‚ã©ã‚“ã©ã‚“é…ããªã‚‹ã—ã€90%ãŒå¤šåˆ†ä¸Šé™ã½ã„\n",
    "* æ­£è¦åŒ–ã‚„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå…¥ã‚ŒãŸã»ã†ãŒç²¾åº¦ã«ã°ã‚‰ã¤ããŒå°‘ãªã„å‚¾å‘ã«ã‚ã‚‹ \n",
    "    ~~~python\n",
    "    self.layer_norm = nn.LayerNorm(D_MODEL)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    ~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f40f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1c69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ©ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n"
     ]
    }
   ],
   "source": [
    "#ãƒ‡ãƒã‚¤ã‚¹ã®é¸æŠ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"åˆ©ç”¨ãƒ‡ãƒã‚¤ã‚¹:\", device)\n",
    "\n",
    "# ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "def accuracy(y, t):\n",
    "    _, argmax_list = torch.max(y, dim=1)\n",
    "    accuracy = sum(argmax_list == t).item()/len(t)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25817adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å ´æ‰€ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "# x: IDãƒ™ã‚¯ãƒˆãƒ«\n",
    "# t: ãƒ©ãƒ™ãƒ«\n",
    "x = np.loadtxt(\"data_90/x_id_vector.csv\", delimiter=\",\")\n",
    "t = np.loadtxt(\"data_90/y_labels.csv\", delimiter=\",\")\n",
    "x = torch.LongTensor(x)\n",
    "t = torch.LongTensor(t)\n",
    "\n",
    "x, x_test, t, t_test = train_test_split(x,t, stratify=t,  random_state=55)\n",
    "\n",
    "x = x.to(device)\n",
    "t = t.to(device)\n",
    "x_test = x_test.to(device)\n",
    "t_test = t_test.to(device)\n",
    "\n",
    "# åˆæœŸè¨­å®š\n",
    "WORDS = 146   # å˜èªæ•°\n",
    "SEQ_LEN = 11  # x.shape[1]  # å…¥åŠ›ã™ã‚‹IDãƒ™ã‚¯ãƒˆãƒ«ã®é•·ã•\n",
    "D_MODEL = 16  # åˆ†æ•£è¡¨ç¾ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ\n",
    "CLASSES = 3   # åˆ†é¡æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49163b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([67, 11]), torch.Size([23, 11]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957f07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, pad_token_id: int=3): # ã“ã“ã§padidng token idã‚’æŒ‡å®šã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "       \n",
    "        #  (1) ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ <pad>ã‚’0ã«è¨­å®š\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=WORDS, embedding_dim=D_MODEL,padding_idx=self.pad_token_id)\n",
    "        \n",
    "        # (2) å­¦ç¿’å¯èƒ½ãªä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆ0ã€œmax_len-1ï¼‰\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=SEQ_LEN, embedding_dim=D_MODEL)\n",
    "        \n",
    "        # Layer Normalization ã¨ Dropout\n",
    "        #self.layer_norm = nn.LayerNorm(D_MODEL)\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # (3) Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=4,\n",
    "            dim_feedforward=32,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,   # [batch, seq, d_model] ã§æ‰±ãˆã‚‹ã‚ˆã†ã«\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n",
    "        \n",
    "        # (4) æ–‡ãƒ™ã‚¯ãƒˆãƒ« â†’ ã‚¯ãƒ©ã‚¹æ•°\n",
    "        self.fc = nn.Linear(in_features=D_MODEL, out_features=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (5) TransformerEncoderã®<pad>ç”¨ãƒã‚¹ã‚¯\n",
    "        # pad ID=3ã§padã®ã¨ã“ã‚ãŒ True ã«ãªã‚‹ã‚ˆã†ã« mask ã‚’ä½œæˆã™ã‚‹\n",
    "        src_key_padding_mask = (x == self.pad_token_id)\n",
    "        \n",
    "        # ---- åŸ‹ã‚è¾¼ã¿ ----\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿\n",
    "        tok_emb = self.token_embedding(x)  # (batch, seq_len=11, d_model=16)    \n",
    "\n",
    "        # ä½ç½®åŸ‹ã‚è¾¼ã¿ \n",
    "        # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã€Œ11ã€ã¯ä½•ã‚’è¡¨ã—ã¦ã„ã‚‹ã®ï¼Ÿ\n",
    "        # æ–‡ã®é•·ã•ãŒ11ã€‚0ã€œ10ã¾ã§ã®æ•°å€¤ã§å˜èªã®ä½ç½®ã‚’ã‚ã‚‰ã‚ã—ã¦ã„ã¾ã™ã€‚\n",
    "        pos_emb = self.pos_embedding(torch.arange(11, device=x.device))  # (seq_len=11, d_model=16)\n",
    "\n",
    "\n",
    "        # (6) åˆ†æ•£è¡¨ç¾è¡Œåˆ—ï¼ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ + ä½ç½®åŸ‹ã‚è¾¼ã¿\n",
    "        x = tok_emb + pos_emb.unsqueeze(0)     # (batch, seq_len=11, d_model=16)\n",
    "\n",
    "               \n",
    "        # (7) Transformer Encoder\n",
    "        h = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)     \n",
    "        \n",
    "        # (8) <BOS>ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå…ˆé ­ï¼‰ã«æƒ…å ±ã‚’é›†ç´„\n",
    "        pooled = h[:, 0, :]  # [batch, d_model]  \n",
    "        y = self.fc(pooled)  # [batch, num_labels=3]    \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37e98da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (token_embedding): Embedding(146, 16, padding_idx=3)\n",
       "  (pos_embedding): Embedding(11, 16)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001a0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82744ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:\tloss:1.074\tacc:0.388\n",
      "19:\tloss:0.735\tacc:0.806\n",
      "29:\tloss:0.305\tacc:0.925\n",
      "39:\tloss:0.171\tacc:0.955\n",
      "49:\tloss:0.067\tacc:1.000\n"
     ]
    }
   ],
   "source": [
    "LOOP = 50\n",
    "\n",
    "for epoch in range(LOOP):\n",
    "    optimizer.zero_grad()\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    acc  = accuracy(y,t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1)%10==0:  \n",
    "        print(f\"{epoch}:\\tloss:{loss.item():.3f}\\tacc:{acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20be0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¤œè¨¼ç²¾åº¦: 0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_test = model(x_test)\n",
    "acc = accuracy(y_test, t_test)\n",
    "print(f\"æ¤œè¨¼ç²¾åº¦: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee3e7e",
   "metadata": {},
   "source": [
    "### æ¤œè¨¼ç²¾åº¦ ãŸã¾ãŸã¾ã»ã¨ã‚“ã©ãŒ70ã€œ80%ã€€æ‚ªã„ã¨60%ã«ãªã‚‹ã\n",
    "* 10å›ãã‚‰ã„è©¦ã—ãŸã‘ã©ã€90%ã¯1å›ã ã‘ã€ã»ã¨ã‚“ã©70ã€œ80%ã®ç¯„å›²ã€‚\n",
    "* ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‰ã«è©¦ã—ãŸã¨ãã¯75%ãã‚‰ã„ã ã£ãŸğŸ˜…\n",
    "* æ•°å€¤ã‚’å¤§ããã™ã‚‹ã¨ã»ã‚“ã®ã™ã“ã—ã ã‘ç²¾åº¦ä¸Šæ˜‡ã£ã½ã„ãŒã€1ã€œ2å€‹å¤šãå½“ãŸã‚‹ã ã‘ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿23å€‹ã ã—ã­ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4dd46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
